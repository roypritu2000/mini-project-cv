{

  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NgSv_lELUv"
      },
      "source": [
        "# Read in text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sIVQNqEP38"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "sample_df = pd.read_csv('etg_samples.txt', sep=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMzTgqvaGvOb"
      },
      "source": [
        "# Minimum Requirement: Detect and Track Cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGCGK-UoGx--"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "model.to('cuda') # comment if no GPU\n",
        "# Open input video: either 1 min version or full version\n",
        "cap = cv2.VideoCapture(\"video_etg_1min.mp4\")\n",
        "\n",
        "# Video properties\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "out = cv2.VideoWriter(\"video_etg_1min_tracked_minimum.mp4\", fourcc, fps, (width, height))\n",
        "\n",
        "CAR_CLASS_ID = 2\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO tracking\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        conf=0.7, # adjustable\n",
        "        classes=[CAR_CLASS_ID],\n",
        "        tracker=\"bytetrack.yaml\",\n",
        "        persist=True\n",
        "    )\n",
        "\n",
        "    for r in results:\n",
        "        if r.boxes.id is None:\n",
        "            continue\n",
        "\n",
        "        boxes = r.boxes.xyxy.cpu().numpy()\n",
        "        track_ids = r.boxes.id.cpu().numpy().astype(int)\n",
        "\n",
        "        for box, track_id in zip(boxes, track_ids):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Draw object ID (instead of confidence)\n",
        "            label = f\"car ID:{track_id}\"\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                label,\n",
        "                (x1, y1 - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.6,\n",
        "                (0, 255, 0),\n",
        "                2\n",
        "            )\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNAe8AwoNNmc"
      },
      "source": [
        "# Perform gaze annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6K4IKlLOGpK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# only keep the first row\n",
        "df_unique = (\n",
        "    sample_df\n",
        "    .sort_values(\"frame_etg\")\n",
        "    .drop_duplicates(subset=\"frame_etg\", keep=\"first\")\n",
        "    .dropna(subset=[\"frame_etg\", \"X\", \"Y\"])\n",
        ")\n",
        "\n",
        "# dictionary containing the frame count and X,Y coordinate only.\n",
        "gaze_dict = {\n",
        "    int(row.frame_etg): (int(row.X), int(row.Y))\n",
        "    for row in df_unique.itertuples()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1r5O66BOkeF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(\"video_etg_1min_tracked_minimum.mp4\")\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "out = cv2.VideoWriter(\"video_etg_1min_tracked_with_gaze_minimum.mp4\", fourcc, fps, (width, height))\n",
        "\n",
        "frame_idx = 0  # OpenCV frame counter\n",
        "\n",
        "def draw_plus(img, center, size=50, thickness=3, color=(0, 0, 255)):\n",
        "    cx, cy = center\n",
        "    half = size // 2\n",
        "    cv2.line(img, (cx - half, cy), (cx + half, cy), color, thickness)\n",
        "    cv2.line(img, (cx, cy - half), (cx, cy + half), color, thickness)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # If gaze data exists for this frame\n",
        "    if frame_idx in gaze_dict:\n",
        "        x, y = gaze_dict[frame_idx]\n",
        "\n",
        "        # Draw red circle (BGR: 0,0,255)\n",
        "        draw_plus(frame, (x, y), size=50, thickness=5, color=(0, 0, 255))\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzmeulGuQ6Xw"
      },
      "source": [
        "# fixation calculation over all objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M8oyGgvQ9fe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Keep only fixation rows\n",
        "df_fix = sample_df[sample_df[\"event_type\"] == \"Fixation\"]\n",
        "\n",
        "# Keep first row per frame\n",
        "df_fix = (\n",
        "    df_fix\n",
        "    .sort_values(\"frame_etg\")\n",
        "    .drop_duplicates(subset=\"frame_etg\", keep=\"first\")\n",
        "    .dropna(subset=[\"frame_etg\", \"X\", \"Y\"])\n",
        ")\n",
        "\n",
        "# Frame → gaze point\n",
        "fixation_dict = {\n",
        "    int(row.frame_etg): (float(row.X), float(row.Y))\n",
        "    for row in df_fix.itertuples()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7F4K0woSYFa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "cap = cv2.VideoCapture(\"video_etg_1min.mp4\")\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "FRAME_TIME = 1.0 / fps  \n",
        "\n",
        "# Accumulators\n",
        "fixation_time = defaultdict(float)      # track_id → seconds\n",
        "track_class = {}                        # track_id → class_name\n",
        "\n",
        "frame_idx = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLdASkF_Ssxe"
      },
      "outputs": [],
      "source": [
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run tracking on all classes\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        persist=True,\n",
        "        conf=0.7,\n",
        "        tracker=\"bytetrack.yaml\"\n",
        "    )\n",
        "\n",
        "    # No fixation in this frame → skip\n",
        "    if frame_idx not in fixation_dict:\n",
        "        frame_idx += 1\n",
        "        continue\n",
        "\n",
        "    gaze_x, gaze_y = fixation_dict[frame_idx]\n",
        "\n",
        "    for r in results:\n",
        "        if r.boxes.id is None:\n",
        "            continue\n",
        "\n",
        "        boxes = r.boxes.xyxy.cpu().numpy()\n",
        "        track_ids = r.boxes.id.cpu().numpy().astype(int)\n",
        "        class_ids = r.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        for box, track_id, cls_id in zip(boxes, track_ids, class_ids):\n",
        "            x1, y1, x2, y2 = box\n",
        "\n",
        "            # Check if fixation lies inside bounding box\n",
        "            if x1 <= gaze_x <= x2 and y1 <= gaze_y <= y2:\n",
        "                fixation_time[track_id] += FRAME_TIME\n",
        "                track_class[track_id] = model.names[cls_id]\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHi1AzZDT18A"
      },
      "outputs": [],
      "source": [
        "output_df = pd.DataFrame([\n",
        "    {\n",
        "        \"object_id\": track_id,\n",
        "        \"class_name\": track_class.get(track_id, \"unknown\"),\n",
        "        \"total_fixation_time_sec\": round(time_sec, 4)\n",
        "    }\n",
        "    for track_id, time_sec in fixation_time.items()\n",
        "])\n",
        "\n",
        "output_df.to_csv(\"video_etg_1min_fixation.txt\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWKVr2479i7"
      },
      "source": [
        "# advanced requirement: instance segmentation with tracking (gaze + RT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKshdg6Y78Aq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "cap = cv2.VideoCapture(\"video_etg.avi\")\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "dt = 1.0 / fps\n",
        "\n",
        "video_writer = cv2.VideoWriter(\n",
        "    \"video_etg_with_real_time_tracking_and_segmentation_advanced.avi\",\n",
        "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "    fps,\n",
        "    (w, h)\n",
        ")\n",
        "\n",
        "model = YOLO(\"yolo11n-seg.pt\")\n",
        "model.to('cuda')  # comment if no GPU\n",
        "class_names = model.names\n",
        "\n",
        "\n",
        "fixation_time = {}  \n",
        "frame_idx = 0\n",
        "\n",
        "ALPHA = 0.35\n",
        "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "FONT_SCALE = 0.6\n",
        "FONT_THICKNESS = 2\n",
        "DASH_LEN = 10\n",
        "GAP_LEN = 6\n",
        "\n",
        "\n",
        "COLOR_PALETTE = [\n",
        "    (0, 255, 0),     # green\n",
        "    (255, 0, 0),     # blue\n",
        "    (0, 0, 255),     # red\n",
        "    (255, 255, 0),   # cyan\n",
        "    (255, 0, 255),   # magenta\n",
        "]\n",
        "\n",
        "NUM_COLORS = len(COLOR_PALETTE)\n",
        "\n",
        "\n",
        "def draw_dashed_rect(img, pt1, pt2, color, thickness):\n",
        "    x1, y1 = pt1\n",
        "    x2, y2 = pt2\n",
        "\n",
        "    for x in range(x1, x2, DASH_LEN + GAP_LEN):\n",
        "        cv2.line(img, (x, y1), (min(x + DASH_LEN, x2), y1), color, thickness)\n",
        "        cv2.line(img, (x, y2), (min(x + DASH_LEN, x2), y2), color, thickness)\n",
        "\n",
        "    for y in range(y1, y2, DASH_LEN + GAP_LEN):\n",
        "        cv2.line(img, (x1, y), (x1, min(y + DASH_LEN, y2)), color, thickness)\n",
        "        cv2.line(img, (x2, y), (x2, min(y + DASH_LEN, y2)), color, thickness)\n",
        "\n",
        "def draw_plus(img, center, size=50, thickness=3, color=(0, 0, 255)):\n",
        "    cx, cy = center\n",
        "    half = size // 2\n",
        "    cv2.line(img, (cx - half, cy), (cx + half, cy), color, thickness)\n",
        "    cv2.line(img, (cx, cy - half), (cx, cy + half), color, thickness)\n",
        "\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        persist=True,\n",
        "        verbose=False,\n",
        "        conf=0.7\n",
        "    )\n",
        "\n",
        "    annotated = frame.copy()\n",
        "    overlay = frame.copy()\n",
        "    r = results[0]\n",
        "\n",
        "    gaze = fixation_dict.get(frame_idx, None)\n",
        "    if gaze is not None:\n",
        "        gx, gy = int(gaze[0]), int(gaze[1])\n",
        "        draw_plus(\n",
        "            annotated,\n",
        "            (gx, gy),\n",
        "            size=100,\n",
        "            thickness=3,\n",
        "            color=(0, 0, 255)\n",
        "        )\n",
        "\n",
        "    if r.boxes is not None and r.boxes.id is not None and r.masks is not None:\n",
        "        boxes = r.boxes.xyxy.cpu().numpy()\n",
        "        ids = r.boxes.id.cpu().numpy().astype(int)\n",
        "        clss = r.boxes.cls.cpu().numpy().astype(int)\n",
        "        polys = r.masks.xy\n",
        "\n",
        "        for box, obj_id, cls_id, poly in zip(boxes, ids, clss, polys):\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "            if obj_id not in fixation_time:\n",
        "                fixation_time[obj_id] = 0.0\n",
        "\n",
        "            color = COLOR_PALETTE[obj_id % NUM_COLORS]\n",
        "\n",
        "            if gaze is not None:\n",
        "                if x1 <= gx <= x2 and y1 <= gy <= y2:\n",
        "                    fixation_time[obj_id] += dt\n",
        "\n",
        "            poly_i = poly.astype(np.int32)\n",
        "            cv2.fillPoly(overlay, [poly_i], color)\n",
        "\n",
        "            cv2.polylines(\n",
        "                annotated,\n",
        "                [poly_i],\n",
        "                isClosed=True,\n",
        "                color=color,\n",
        "                thickness=2\n",
        "            )\n",
        "\n",
        "\n",
        "            draw_dashed_rect(\n",
        "                annotated,\n",
        "                (x1, y1),\n",
        "                (x2, y2),\n",
        "                color,\n",
        "                thickness=2\n",
        "            )\n",
        "\n",
        "            class_name = class_names[cls_id].upper()\n",
        "            fixation_ms = int(fixation_time[obj_id] * 1000)\n",
        "\n",
        "            line1 = f\"{class_name} ID: {obj_id}\"\n",
        "            line2 = f\"{fixation_ms} ms\"\n",
        "\n",
        "            (w1, h1), _ = cv2.getTextSize(line1, FONT, FONT_SCALE, FONT_THICKNESS)\n",
        "            (w2, h2), _ = cv2.getTextSize(line2, FONT, FONT_SCALE, FONT_THICKNESS)\n",
        "\n",
        "            box_w = max(w1, w2) + 20\n",
        "            box_h = h1 + h2 + 20\n",
        "\n",
        "            cx = (x1 + x2) // 2\n",
        "            box_x1 = int(cx - box_w / 2)\n",
        "            box_y1 = max(y1 - box_h - 8, 0)\n",
        "            box_x2 = box_x1 + box_w\n",
        "            box_y2 = box_y1 + box_h\n",
        "\n",
        "            cv2.rectangle(\n",
        "                annotated,\n",
        "                (box_x1, box_y1),\n",
        "                (box_x2, box_y2),\n",
        "                color,\n",
        "                thickness=-1\n",
        "            )\n",
        "\n",
        "            text_x1 = box_x1 + (box_w - w1) // 2\n",
        "            text_y1 = box_y1 + h1 + 8\n",
        "\n",
        "            text_x2 = box_x1 + (box_w - w2) // 2\n",
        "            text_y2 = text_y1 + h2 + 4\n",
        "\n",
        "            cv2.putText(\n",
        "                annotated,\n",
        "                line1,\n",
        "                (text_x1, text_y1),\n",
        "                FONT,\n",
        "                FONT_SCALE,\n",
        "                (255, 255, 255),\n",
        "                FONT_THICKNESS\n",
        "            )\n",
        "\n",
        "            cv2.putText(\n",
        "                annotated,\n",
        "                line2,\n",
        "                (text_x2, text_y2),\n",
        "                FONT,\n",
        "                FONT_SCALE,\n",
        "                (255, 255, 255),\n",
        "                FONT_THICKNESS\n",
        "            )\n",
        "\n",
        "    annotated = cv2.addWeighted(\n",
        "        overlay,\n",
        "        ALPHA,\n",
        "        annotated,\n",
        "        1 - ALPHA,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    video_writer.write(annotated)\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
